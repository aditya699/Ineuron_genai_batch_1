{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b44fcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AppleSupport causing the reply to be disregar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@105835 Your business means a lot to us. Pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@76328 I really hope you all change but I'm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@105836 LiveChat is online at the moment - htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginTrains see attached error message. I've...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  @AppleSupport causing the reply to be disregar...\n",
       "1  @105835 Your business means a lot to us. Pleas...\n",
       "2  @76328 I really hope you all change but I'm su...\n",
       "3  @105836 LiveChat is online at the moment - htt...\n",
       "4  @VirginTrains see attached error message. I've..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "#Due to time constraint/Compute constraints will do in sample and the test for fulldatset\n",
    "data=pd.read_csv(\"C:/Users/AdityaBhatt/Desktop/sample.csv\")\n",
    "data=data[['text']]\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a8c791f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell_checker = SpellChecker()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        \n",
    "        # Dictionary of common abbreviations and their full forms\n",
    "        abbreviations = {\n",
    "            \"u\": \"you\",\n",
    "            \"r\": \"are\",\n",
    "            \"thx\": \"thanks\",\n",
    "            \"lol\": \"laugh out loud\",\n",
    "            \"brb\": \"be right back\",\n",
    "            \"btw\": \"by the way\",\n",
    "            \"afaik\": \"as far as I know\",\n",
    "            \"imho\": \"in my humble opinion\",\n",
    "            \"omg\": \"oh my god\",\n",
    "            \"fyi\": \"for your information\",\n",
    "            \"tbh\": \"to be honest\",\n",
    "            \"idk\": \"I don't know\",\n",
    "            \"yolo\": \"you only live once\",\n",
    "            \"gtg\": \"got to go\",\n",
    "            \"hmu\": \"hit me up\",\n",
    "            \"tmi\": \"too much information\",\n",
    "            \"bff\": \"best friends forever\",\n",
    "            \"icymi\": \"in case you missed it\",\n",
    "            \"irl\": \"in real life\",\n",
    "            \"fomo\": \"fear of missing out\",\n",
    "            \"np\": \"no problem\",\n",
    "            \"omw\": \"on my way\",\n",
    "            \"smh\": \"shaking my head\",\n",
    "            \"rofl\": \"rolling on the floor laughing\",\n",
    "            \"imo\": \"in my opinion\",\n",
    "            \"tbt\": \"throwback Thursday\",\n",
    "            \"wcw\": \"woman crush Wednesday\",\n",
    "            \"mcm\": \"man crush Monday\",\n",
    "            \"ootd\": \"outfit of the day\",\n",
    "            \"hbd\": \"happy birthday\",\n",
    "            \"tgif\": \"thank goodness it's Friday\",\n",
    "            \"nsfw\": \"not safe for work\",\n",
    "            \"btw\": \"by the way\",\n",
    "            \"afaik\": \"as far as I know\",\n",
    "            \"icymi\": \"in case you missed it\",\n",
    "            \"yolo\": \"you only live once\",\n",
    "            \"fomo\": \"fear of missing out\",\n",
    "            \"tmi\": \"too much information\",\n",
    "            \"imo\": \"in my opinion\",\n",
    "            # Add more abbreviations as needed\n",
    "        }\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        # Replace abbreviations\n",
    "        tokens = [abbreviations.get(word, word) for word in tokens]\n",
    "        # Stemming\n",
    "        tokens = [self.stemmer.stem(word) for word in tokens]\n",
    "        #Lemmatization\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        # Spelling correction\n",
    "        corrected_tokens = [self.spell_checker.correction(word) if word is not None else None for word in tokens]\n",
    "        corrected_tokens = [word for word in corrected_tokens if word is not None]\n",
    "        \n",
    "        text = ' '.join(corrected_tokens)\n",
    "        \n",
    "        \n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Convert to uppercase (optional)\n",
    "        # text = text.upper()\n",
    "        \n",
    "         # Remove emojis\n",
    "        text = emoji.demojize(text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "         # Handle white spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        return text\n",
    "        \n",
    "        \n",
    "\n",
    "    def apply_cleaning(self):\n",
    "        # Apply the clean_text function to the 'text' column\n",
    "        self.data['cleaned_text'] = self.data['text'].apply(self.clean_text)\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "\n",
    "clean_data=TextPreprocessing(data)\n",
    "clean_data=pd.DataFrame(clean_data.apply_cleaning())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb96eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=clean_data[['cleaned_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe643d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>applesupport caus repli disregard tap notif ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>busi mean lot u plea dm name zip code addit de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realli hope chang im sure wont dont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>livechat onlin moment contact option leav mess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virgintrain see attach error messag ive tri le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>wish amazon option get ship up store avoid lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>reschedul shit tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>hey sara sorri hear issu ask lay speed websit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>tesco bit find layout cumbersom remov item fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>doesnt help plea dm full name address email in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cleaned_text\n",
       "0   applesupport caus repli disregard tap notif ke...\n",
       "1   busi mean lot u plea dm name zip code addit de...\n",
       "2                 realli hope chang im sure wont dont\n",
       "3   livechat onlin moment contact option leav mess...\n",
       "4   virgintrain see attach error messag ive tri le...\n",
       "..                                                ...\n",
       "88  wish amazon option get ship up store avoid lot...\n",
       "89                            reschedul shit tomorrow\n",
       "90  hey sara sorri hear issu ask lay speed websit ...\n",
       "91  tesco bit find layout cumbersom remov item fav...\n",
       "92  doesnt help plea dm full name address email in...\n",
       "\n",
       "[93 rows x 1 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada73990",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a19cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_text  ac  account  ad  addit  \\\n",
      "0   applesupport caus repli disregard tap notif ke...   0        0   0      0   \n",
      "1   busi mean lot u plea dm name zip code addit de...   0        0   0      1   \n",
      "2                 realli hope chang im sure wont dont   0        0   0      0   \n",
      "3   livechat onlin moment contact option leav mess...   0        0   0      0   \n",
      "4   virgintrain see attach error messag ive tri le...   0        0   0      0   \n",
      "..                                                ...  ..      ...  ..    ...   \n",
      "88  wish amazon option get ship up store avoid lot...   0        0   0      0   \n",
      "89                            reschedul shit tomorrow   0        0   0      0   \n",
      "90  hey sara sorri hear issu ask lay speed websit ...   0        0   0      0   \n",
      "91  tesco bit find layout cumbersom remov item fav...   0        0   0      0   \n",
      "92  doesnt help plea dm full name address email in...   0        0   0      0   \n",
      "\n",
      "    address  adnam  advis  affect  age  ...  work  wtf  xfiniti  yall  yard  \\\n",
      "0         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "1         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "2         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "3         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "4         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "..      ...    ...    ...     ...  ...  ...   ...  ...      ...   ...   ...   \n",
      "88        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "89        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "90        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "91        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "92        1      0      0       0    0  ...     0    0        0     0     0   \n",
      "\n",
      "    yep  youll  your  youv  zip  \n",
      "0     0      0     0     0    0  \n",
      "1     0      0     0     0    1  \n",
      "2     0      0     0     0    0  \n",
      "3     0      0     0     0    0  \n",
      "4     0      0     0     0    0  \n",
      "..  ...    ...   ...   ...  ...  \n",
      "88    0      0     0     0    0  \n",
      "89    0      0     0     0    0  \n",
      "90    0      0     0     0    0  \n",
      "91    0      0     0     0    0  \n",
      "92    0      0     0     0    0  \n",
      "\n",
      "[93 rows x 465 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit and transform the 'cleaned_text' column\n",
    "one_hot_encoded = vectorizer.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded features\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the one-hot encoded features with the original DataFrame\n",
    "clean_data_encoded =pd.DataFrame(pd.concat([clean_data, one_hot_df], axis=1))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(clean_data_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8431cf1",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9dc29eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_text  ac  account  ad  addit  \\\n",
      "0   applesupport caus repli disregard tap notif ke...   0        0   0      0   \n",
      "1   busi mean lot u plea dm name zip code addit de...   0        0   0      1   \n",
      "2                 realli hope chang im sure wont dont   0        0   0      0   \n",
      "3   livechat onlin moment contact option leav mess...   0        0   0      0   \n",
      "4   virgintrain see attach error messag ive tri le...   0        0   0      0   \n",
      "..                                                ...  ..      ...  ..    ...   \n",
      "88  wish amazon option get ship up store avoid lot...   0        0   0      0   \n",
      "89                            reschedul shit tomorrow   0        0   0      0   \n",
      "90  hey sara sorri hear issu ask lay speed websit ...   0        0   0      0   \n",
      "91  tesco bit find layout cumbersom remov item fav...   0        0   0      0   \n",
      "92  doesnt help plea dm full name address email in...   0        0   0      0   \n",
      "\n",
      "    address  adnam  advis  affect  age  ...  work  wtf  xfiniti  yall  yard  \\\n",
      "0         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "1         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "2         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "3         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "4         0      0      0       0    0  ...     0    0        0     0     0   \n",
      "..      ...    ...    ...     ...  ...  ...   ...  ...      ...   ...   ...   \n",
      "88        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "89        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "90        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "91        0      0      0       0    0  ...     0    0        0     0     0   \n",
      "92        1      0      0       0    0  ...     0    0        0     0     0   \n",
      "\n",
      "    yep  youll  your  youv  zip  \n",
      "0     0      0     0     0    0  \n",
      "1     0      0     0     0    1  \n",
      "2     0      0     0     0    0  \n",
      "3     0      0     0     0    0  \n",
      "4     0      0     0     0    0  \n",
      "..  ...    ...   ...   ...  ...  \n",
      "88    0      0     0     0    0  \n",
      "89    0      0     0     0    0  \n",
      "90    0      0     0     0    0  \n",
      "91    0      0     0     0    0  \n",
      "92    0      0     0     0    0  \n",
      "\n",
      "[93 rows x 465 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit and transform the 'cleaned_text' column\n",
    "one_hot_encoded = vectorizer.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded features\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the one-hot encoded features with the original DataFrame\n",
    "clean_data_encoded = pd.concat([clean_data, one_hot_df], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(clean_data_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0cfa9",
   "metadata": {},
   "source": [
    "# N GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba0ce391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_text  account as  \\\n",
      "0   applesupport caus repli disregard tap notif ke...           0   \n",
      "1   busi mean lot u plea dm name zip code addit de...           0   \n",
      "2                 realli hope chang im sure wont dont           0   \n",
      "3   livechat onlin moment contact option leav mess...           0   \n",
      "4   virgintrain see attach error messag ive tri le...           0   \n",
      "..                                                ...         ...   \n",
      "88  wish amazon option get ship up store avoid lot...           0   \n",
      "89                            reschedul shit tomorrow           0   \n",
      "90  hey sara sorri hear issu ask lay speed websit ...           0   \n",
      "91  tesco bit find layout cumbersom remov item fav...           0   \n",
      "92  doesnt help plea dm full name address email in...           0   \n",
      "\n",
      "    account inform  account prior  addit detail  address email  address name  \\\n",
      "0                0              0             0              0             0   \n",
      "1                0              0             1              0             0   \n",
      "2                0              0             0              0             0   \n",
      "3                0              0             0              0             0   \n",
      "4                0              0             0              0             0   \n",
      "..             ...            ...           ...            ...           ...   \n",
      "88               0              0             0              0             0   \n",
      "89               0              0             0              0             0   \n",
      "90               0              0             0              0             0   \n",
      "91               0              0             0              0             0   \n",
      "92               0              0             0              1             0   \n",
      "\n",
      "    adnam broadsid  affect playback  age load  ...  yep ive tri laptop  \\\n",
      "0                0                0         0  ...                   0   \n",
      "1                0                0         0  ...                   0   \n",
      "2                0                0         0  ...                   0   \n",
      "3                0                0         0  ...                   0   \n",
      "4                0                0         0  ...                   0   \n",
      "..             ...              ...       ...  ...                 ...   \n",
      "88               0                0         0  ...                   0   \n",
      "89               0                0         0  ...                   0   \n",
      "90               0                0         0  ...                   0   \n",
      "91               0                0         0  ...                   0   \n",
      "92               0                0         0  ...                   0   \n",
      "\n",
      "    youll even receiv congrat  youll still full basket  \\\n",
      "0                           0                        0   \n",
      "1                           0                        0   \n",
      "2                           0                        0   \n",
      "3                           0                        0   \n",
      "4                           0                        0   \n",
      "..                        ...                      ...   \n",
      "88                          0                        0   \n",
      "89                          0                        0   \n",
      "90                          0                        0   \n",
      "91                          0                        0   \n",
      "92                          0                        0   \n",
      "\n",
      "    your experienc dm well  your use thatd great  your use use specif  \\\n",
      "0                        0                     0                    0   \n",
      "1                        0                     0                    0   \n",
      "2                        0                     0                    0   \n",
      "3                        0                     0                    0   \n",
      "4                        0                     0                    0   \n",
      "..                     ...                   ...                  ...   \n",
      "88                       0                     0                    0   \n",
      "89                       0                     0                    0   \n",
      "90                       0                     0                    0   \n",
      "91                       0                     0                    0   \n",
      "92                       0                     0                    0   \n",
      "\n",
      "    your welcom there anyth  youv paralys phone updat  \\\n",
      "0                         0                         0   \n",
      "1                         0                         0   \n",
      "2                         0                         0   \n",
      "3                         0                         0   \n",
      "4                         0                         0   \n",
      "..                      ...                       ...   \n",
      "88                        0                         0   \n",
      "89                        0                         0   \n",
      "90                        0                         0   \n",
      "91                        0                         0   \n",
      "92                        0                         0   \n",
      "\n",
      "    youv tri sinc experienc  zip code addit detail  \n",
      "0                         0                      0  \n",
      "1                         0                      1  \n",
      "2                         0                      0  \n",
      "3                         0                      0  \n",
      "4                         0                      0  \n",
      "..                      ...                    ...  \n",
      "88                        0                      0  \n",
      "89                        0                      0  \n",
      "90                        0                      0  \n",
      "91                        0                      0  \n",
      "92                        0                      0  \n",
      "\n",
      "[93 rows x 2149 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create instances of CountVectorizer for n-grams of lengths 2, 3, and 4\n",
    "vectorizer_2grams = CountVectorizer(ngram_range=(2, 2))\n",
    "vectorizer_3grams = CountVectorizer(ngram_range=(3, 3))\n",
    "vectorizer_4grams = CountVectorizer(ngram_range=(4, 4))\n",
    "\n",
    "# Fit and transform the 'cleaned_text' column for each n-gram length\n",
    "ngrams_2grams = vectorizer_2grams.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "ngrams_3grams = vectorizer_3grams.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "ngrams_4grams = vectorizer_4grams.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "\n",
    "# Create DataFrames with the n-gram features for each length\n",
    "ngrams_df_2grams = pd.DataFrame(ngrams_2grams, columns=vectorizer_2grams.get_feature_names_out())\n",
    "ngrams_df_3grams = pd.DataFrame(ngrams_3grams, columns=vectorizer_3grams.get_feature_names_out())\n",
    "ngrams_df_4grams = pd.DataFrame(ngrams_4grams, columns=vectorizer_4grams.get_feature_names_out())\n",
    "\n",
    "# Concatenate the n-gram features with the original DataFrame\n",
    "clean_data_ngrams = pd.concat([clean_data, ngrams_df_2grams, ngrams_df_3grams, ngrams_df_4grams], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(clean_data_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360038b",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25788e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_text   ac  account   ad  \\\n",
      "0   applesupport caus repli disregard tap notif ke...  0.0      0.0  0.0   \n",
      "1   busi mean lot u plea dm name zip code addit de...  0.0      0.0  0.0   \n",
      "2                 realli hope chang im sure wont dont  0.0      0.0  0.0   \n",
      "3   livechat onlin moment contact option leav mess...  0.0      0.0  0.0   \n",
      "4   virgintrain see attach error messag ive tri le...  0.0      0.0  0.0   \n",
      "..                                                ...  ...      ...  ...   \n",
      "88  wish amazon option get ship up store avoid lot...  0.0      0.0  0.0   \n",
      "89                            reschedul shit tomorrow  0.0      0.0  0.0   \n",
      "90  hey sara sorri hear issu ask lay speed websit ...  0.0      0.0  0.0   \n",
      "91  tesco bit find layout cumbersom remov item fav...  0.0      0.0  0.0   \n",
      "92  doesnt help plea dm full name address email in...  0.0      0.0  0.0   \n",
      "\n",
      "       addit  address  adnam  advis  affect  age  ...  work  wtf  xfiniti  \\\n",
      "0   0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "1   0.322229  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "2   0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "3   0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "4   0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "..       ...      ...    ...    ...     ...  ...  ...   ...  ...      ...   \n",
      "88  0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "89  0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "90  0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "91  0.000000  0.00000    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "92  0.000000  0.28821    0.0    0.0     0.0  0.0  ...   0.0  0.0      0.0   \n",
      "\n",
      "    yall  yard  yep  youll  your  youv       zip  \n",
      "0    0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "1    0.0   0.0  0.0    0.0   0.0   0.0  0.322229  \n",
      "2    0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "3    0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "4    0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "..   ...   ...  ...    ...   ...   ...       ...  \n",
      "88   0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "89   0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "90   0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "91   0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "92   0.0   0.0  0.0    0.0   0.0   0.0  0.000000  \n",
      "\n",
      "[93 rows x 465 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'cleaned_text' column\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(clean_data['cleaned_text']).toarray()\n",
    "\n",
    "# Create a DataFrame with the TF-IDF features\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF features with the original DataFrame\n",
    "clean_data_tfidf = pd.concat([clean_data, tfidf_df], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(clean_data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abba15",
   "metadata": {},
   "source": [
    "# ONE HOT ENCODING VS BOW VS N GRAM VS TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5b13",
   "metadata": {},
   "source": [
    "Certainly! Here are five advantages and five disadvantages for each of the following text representation techniques: One-Hot Encoding (OHE), Bag of Words (BoW), N-grams, and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "### One-Hot Encoding (OHE):\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Simple Representation:** OHE is a straightforward and easy-to-understand representation, where each word is represented by a binary value (0 or 1).\n",
    "\n",
    "2. **No Information Loss:** It maintains the information about the presence or absence of words in a document without losing any information.\n",
    "\n",
    "3. **Independence:** OHE treats each word independently, making it suitable for certain tasks where word order is less important.\n",
    "\n",
    "4. **No Weighting:** There is no weighting of words, making it useful when equal importance is assigned to all words.\n",
    "\n",
    "5. **Interpretability:** The resulting matrix is interpretable, making it easy to inspect and understand the features.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **High Dimensionality:** OHE results in a high-dimensional sparse matrix, especially when dealing with a large vocabulary, which can be computationally expensive.\n",
    "\n",
    "2. **Lack of Semantic Information:** OHE does not capture semantic relationships between words; it treats all words as independent features.\n",
    "\n",
    "3. **No Contextual Information:** It doesn't consider the context in which words appear, making it less effective in capturing meaning.\n",
    "\n",
    "4. **Memory Consumption:** The high number of features can lead to increased memory consumption.\n",
    "\n",
    "5. **Not Suitable for Large Texts:** OHE may not be practical for large texts due to the resulting large and sparse feature space.\n",
    "\n",
    "### Bag of Words (BoW):\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Simplicity:** BoW is simple and easy to implement, making it a good baseline representation.\n",
    "\n",
    "2. **No Prior Knowledge Required:** BoW doesn't require prior knowledge of the language or domain-specific information.\n",
    "\n",
    "3. **Flexibility:** It can be applied to various NLP tasks, including sentiment analysis, classification, and clustering.\n",
    "\n",
    "4. **Efficient Storage:** The sparse nature of the BoW representation makes it memory-efficient.\n",
    "\n",
    "5. **Interpretability:** Similar to OHE, the resulting matrix is interpretable.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Loss of Word Order:** BoW discards the order of words, losing some syntactic and semantic information.\n",
    "\n",
    "2. **Equal Weight for All Words:** It treats all words equally, ignoring variations in importance.\n",
    "\n",
    "3. **High Dimensionality:** Similar to OHE, BoW can result in a high-dimensional feature space.\n",
    "\n",
    "4. **No Semantic Information:** BoW lacks the ability to capture semantic relationships between words.\n",
    "\n",
    "5. **Difficulty with Synonyms:** It struggles to capture synonym relationships, as each word is treated independently.\n",
    "\n",
    "### N-grams:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Captures Local Context:** N-grams capture local context and word dependencies, providing more information about word relationships.\n",
    "\n",
    "2. **Flexible Context Modeling:** N-grams allow flexibility in capturing context by considering sequences of adjacent words.\n",
    "\n",
    "3. **Improved Representation:** They can capture partial word ordering and provide a more detailed representation of the text.\n",
    "\n",
    "4. **Contextual Information:** N-grams can capture some level of contextual information, especially with larger values of N.\n",
    "\n",
    "5. **Effective for Short Texts:** N-grams can be effective for short texts or when word order is important.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Increased Dimensionality:** As N increases, the dimensionality of the feature space grows exponentially.\n",
    "\n",
    "2. **Sparse Data:** For larger values of N, the data can become sparse, leading to increased computational requirements.\n",
    "\n",
    "3. **Loss of Global Context:** N-grams may lose global context and struggle to capture long-range dependencies.\n",
    "\n",
    "4. **Not Suitable for All Tasks:** While effective for certain tasks, N-grams may not be suitable for tasks where global context is crucial.\n",
    "\n",
    "5. **Data Sparsity:** Sparse data can be challenging to handle and may require additional techniques.\n",
    "\n",
    "### TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Weighting of Words:** TF-IDF introduces term weighting, considering both term frequency and inverse document frequency.\n",
    "\n",
    "2. **Contextual Information:** It captures contextual information by assigning higher weights to important terms.\n",
    "\n",
    "3. **Reduced Impact of Stopwords:** Common words (stopwords) are down-weighted, reducing their impact on the representation.\n",
    "\n",
    "4. **Global Context:** TF-IDF considers both local and global context, providing a balance between the two.\n",
    "\n",
    "5. **Effective for Long Documents:** TF-IDF can be effective for representing long documents where word order may be less critical.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Complexity:** TF-IDF introduces additional complexity compared to simpler representations like BoW.\n",
    "\n",
    "2. **Difficulty with Synonyms:** It may still struggle with synonyms, as the focus is on term frequency and document frequency.\n",
    "\n",
    "3. **Loss of Word Order:** Like BoW, TF-IDF doesn't capture word order information.\n",
    "\n",
    "4. **Not Suitable for Short Texts:** In some cases, TF-IDF may not perform well on very short texts.\n",
    "\n",
    "5. **Requires Document Frequency Information:** Document frequency information is needed, which may not be readily available for all texts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
